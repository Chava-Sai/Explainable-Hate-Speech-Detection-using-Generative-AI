{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp105SOU04tW"
      },
      "source": [
        "# Fine-Tuning the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiXyIHqa08zU"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXrTc_VB0_KX"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "# >>> Device: cuda\n",
        "# >>> Tesla T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "runHRtgy1CN8"
      },
      "outputs": [],
      "source": [
        "# Hugging face login\n",
        "token = '<hf-token>'\n",
        "\n",
        "orig_model_path = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "# model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "                                load_in_4bit=True,\n",
        "                                bnb_4bit_use_double_quant=True,\n",
        "                                bnb_4bit_quant_type=\"nf4\",\n",
        "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                               )\n",
        "model = AutoModelForCausalLM.from_pretrained(orig_model_path, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(orig_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-EJPi1z1JW-"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "LLM = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-YGIrJ31LQ2"
      },
      "outputs": [],
      "source": [
        "text = \"tell me about Indian Population (Give the related information and if you dont know the asnwer then tell me you dont know about it)\"\n",
        "LLM.invoke(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCN1gvUy1SKw"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PBXvPS81Q98"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    #model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
        "    model_name = \"efederici/sentence-bert-base\", # Sentence Bert Base\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lzAM8i-1VqN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "\n",
        "# Read data from Excel file\n",
        "dataset = pd.read_excel('all_topics_vectordb.xlsx') # Resources Scrapped from the Internet\n",
        "db_docs = dataset['Data'].astype(str).tolist()\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "# Create FAISS VectorStore from texts and embeddings\n",
        "vector_db = FAISS.from_texts(db_docs, embeddings)\n",
        "\n",
        "# Save VectorStore to binary format\n",
        "vectorDB_filename = \"vectorDB.bin\"\n",
        "with open(vectorDB_filename, \"wb\") as f:\n",
        "    pickle.dump(vector_db, f)\n",
        "print(\"----------------\")\n",
        "print(\"VectorDB Stored\")\n",
        "print(\"----------------\\n\")\n",
        "\n",
        "# Load VectorStore from binary format\n",
        "with open(vectorDB_filename, \"rb\") as f:\n",
        "    loaded_vector_db = pickle.load(f)\n",
        "print(\"\\n----------------\")\n",
        "print(\"VectorDB Loaded\")\n",
        "print(\"----------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RqeIDnI1eDL"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "\n",
        "# Create embeddings object\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "vector_db = FAISS.from_texts(db_docs, embeddings)\n",
        "retriever = VectorStoreRetriever(vectorstore=vector_db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJBh8DzM1gHw"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a excellent AI assistant and you know the world's knowledge.Now i want you to retrieve the relavent splits from the VectorDB according to the given query then summarize that relavent splits and print a conclusion from the context that we got from VectorDB.\n",
        "              {context}\n",
        "              If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "              Chat history: {history}\n",
        "              Question: {question}\n",
        "              Write your answers short. Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "        template=template, input_variables=[\"history\", \"context\", \"question\"]\n",
        "    )\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "        llm=LLM,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs={\n",
        "            \"verbose\": False,\n",
        "            \"prompt\": prompt,\n",
        "            \"memory\": ConversationBufferMemory(\n",
        "                memory_key=\"history\",\n",
        "                input_key=\"question\"),\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wPIvHXT1kPL"
      },
      "source": [
        "# Getting Reasons form the VectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swdnqxof1hxh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Read data from Excel file\n",
        "dataset = pd.read_excel('ML_Project_main_hate_fake.xlsx') # 25 records data from claim from main dataset\n",
        "\n",
        "# Convert data to list of strings\n",
        "db_docs = dataset['Pre_Processed_English_text'].astype(str).tolist()\n",
        "print(\"No.of Records : \", len(db_docs))\n",
        "# Iterate through each document, get RAG response, and store reasoning in new column\n",
        "reasons = []\n",
        "total_docs = len(dataset)\n",
        "with tqdm(total=total_docs, desc=\"Processing documents\") as pbar:\n",
        "    for index, row in dataset.iterrows():\n",
        "        query = row['Pre_Processed_English_text']\n",
        "        rag_response = qa.run(query)\n",
        "        reasons.append(rag_response)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Add new column 'Reason' to the dataset\n",
        "dataset['Reason'] = reasons\n",
        "\n",
        "# Save the dataset with the name \"RAG_With_Reasons\"\n",
        "dataset.to_excel(\"ML_Projects_With_RAG_Reasons.xlsx\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "E9xqyjpwVeJX",
        "ZsIuTMcOVqzR",
        "v7qqcrAUuiRU"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
